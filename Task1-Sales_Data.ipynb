{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As the original sales dataset file size exceeds over 4.5+Gb and has 46million rows,Lets just take 1st 500000 rows and create it as a new dataset.",
   "id": "77870c432c24d335"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = r\"D:\\Dataset\\Flipkart-Sales-Dataset\\Sales.csv\"\n",
    "output_file=r\"D:\\Dataset\\Flipkart-Sales-Dataset\\Sales_sample.csv\"\n",
    "sample_size=500000\n",
    "\n",
    "df_sample = pd.read_csv(input_file,nrows=sample_size)\n",
    "df_sample=df_sample.drop_duplicates()\n",
    "\n",
    "df_sample.to_csv(output_file,index=False)\n"
   ],
   "id": "5decddde54d538d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "import dataset and review basic infos\n",
   "id": "6aabae2749dc2677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path=r\"D:\\Dataset\\Flipkart-Sales-Dataset\\Sales_sample.csv\"\n",
    "df=pd.read_csv(file_path)\n",
    "df_cleaned=df.copy()\n",
    "\n",
    "#basic infos\n",
    "print(\"First 10 rows:\")\n",
    "print(df.head(10))\n",
    "print(\"\\nShape (rows,columns):)\",df.shape)\n",
    "print(\"\\nColumns:\",df.columns.tolist())\n",
    "print(\"\\nQuick summary of types and missing values:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic statistics for numeric columns:\")\n",
    "print(df.describe())\n",
    "print(\"\\nHow many missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nShow me a few random rows to peek at the data:\")\n",
    "print(df.sample())\n"
   ],
   "id": "9101b5629c79ebf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Identify and handle missing values using .isnull() in Python or filters in Excel.",
   "id": "4763102ba989365c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Missing values per column:\\n\",df.isnull().sum())#before\n",
    "\n",
    "#as the number of missing values of the column-total_weighted_landing_price is less than 5% of the dataset.So removing the rows with missing values\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"total_weighted_landing_price\"])\n",
    "\n",
    "print(\"Missing values per column:\\n\",df_cleaned.isnull().sum())#after\n"
   ],
   "id": "fc99d354c40cb28b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove duplicate rows using .drop_duplicates() or Excel’s “Remove Duplicates”.",
   "id": "f78ab60dbaa321b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Number of duplicates: {df.duplicated().sum()}\")\n",
    "#Even thought the no of duplicates is 0.Just using drop duplicates.\n",
    "df_cleaned.drop_duplicates()"
   ],
   "id": "cb712b2456b21f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Standardize text values like gender, country names, etc.",
   "id": "908f7ac4d5f1944c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_city = df[\"city_name\"].unique()\n",
    "print(f\"Number of unique cities: {len(unique_city)}\")\n",
    "print(unique_city)\n",
    "\n",
    "city_counts=df[\"city_name\"].value_counts()\n",
    "print(city_counts)#before\n",
    "\n",
    "df_cleaned[\"city_name\"]=df_cleaned[\"city_name\"].replace({\"HR-NCR\":\"Delhi\"})#Since HR-NCR (like Gurugram or Faridabad) is part of the National Capital Region and economically linked to Delhi, merging it under “Delhi” to simplify reporting.\n",
    "df_cleaned[\"city_name\"] =df_cleaned[\"city_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "unique_city = df_cleaned[\"city_name\"].unique()\n",
    "print(f\"Number of unique cities: {len(unique_city)}\")\n",
    "print(unique_city)\n",
    "\n",
    "city_counts=df_cleaned[\"city_name\"].value_counts()\n",
    "print(city_counts)#after\n",
    "print(df_cleaned[\"city_name\"])\n"
   ],
   "id": "504813659483063d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert date formats to a consistent type (e.g., dd-mm-yyyy).",
   "id": "54448b9e0a6864dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df[\"date_\"].head(10))#before\n",
    "\n",
    "df_cleaned[\"date_\"]=pd.to_datetime(df_cleaned[\"date_\"],errors=\"coerce\")#Convert all date values to proper datetime\n",
    "df_cleaned[\"date_\"]=df_cleaned[\"date_\"].dt.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# invalid_dates = df_cleaned[df_cleaned[\"date_\"].isna()]#to find invalid dates\n",
    "# print(\"Invalid dates:\",invalid_dates[[\"date_\"]])\n",
    "\n",
    "\n",
    "print(df_cleaned[\"date_\"].head(10))#after\n",
    "print(df_cleaned['date_'].dtypes)\n",
    "# print(df[\"date_\"].tail(20))"
   ],
   "id": "7c40c384d7a16d28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Rename column headers to be clean and uniform (e.g., lowercase, no spaces).",
   "id": "ea59e3794e7f6cf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Before\",df.columns.tolist())\n",
    "\n",
    "df_cleaned=df_cleaned.rename(columns={\"date_\":\"date\"})\n",
    "\n",
    "df_cleaned.columns=df_cleaned.columns.astype(str).str.strip().str.upper()\n",
    "print(\"After\",df_cleaned.columns.tolist())"
   ],
   "id": "ca3d04585930be74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check and fix data types (e.g., age should be int, date as datetime).",
   "id": "7b6137c1ec25a6c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Datatypes:\\n\",df.dtypes)\n",
    "print(\"Datatypes cleaned\\n:\",df_cleaned.dtypes)"
   ],
   "id": "e55e3042775e8eaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "removing unwanted columns\n",
   "id": "808a23ab0e991237"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_cleaned=df_cleaned.drop(columns=['UNNAMED: 0.2', 'UNNAMED: 0.1', 'UNNAMED: 0'])\n",
    "print(df_cleaned.head())"
   ],
   "id": "36daaf6820611d80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the cleaned dataset as new csv file\n",
   "id": "79088ebef5b7e0bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "export_path = r\"D:\\Dataset\\Flipkart-Sales-Dataset\\Sales_sample_cleaned.csv\"\n",
    "# Save to CSV\n",
    "df_cleaned.to_csv(export_path)\n",
    "print(f\"Cleaned CSV saved at: {export_path}\")"
   ],
   "id": "21701b17de9c1ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "testing\n",
   "id": "907388cc399a1b5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"First 10 rows:\")\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\"\\nShape (rows,columns):\", df_cleaned.shape)\n",
    "\n",
    "print(\"\\nColumns:\", df_cleaned.columns.tolist())\n",
    "\n",
    "print(\"\\nQuick summary of types and missing values:\")\n",
    "print(df_cleaned.info())\n",
    "\n",
    "print(\"\\nBasic statistics for numeric columns:\")\n",
    "print(df_cleaned.describe())\n",
    "\n",
    "print(\"\\nHow many missing values per column:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "print(\"\\nShow me a few random rows to peek at the data:\")\n",
    "print(df_cleaned.sample())\n"
   ],
   "id": "cbdd1b0466392b85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning Summary for Flipkart Sales Dataset\n",
    "\n",
    "- Original dataset was over 4.5 GB with 46 million rows. Sampled first 500,000 rows for manageable processing.\n",
    "- Removed duplicate rows using `.drop_duplicates()`.\n",
    "- Identified and dropped rows with missing values in `total_weighted_landing_price` (less than 5% missing).\n",
    "- Standardized city names: merged `HR-NCR` under `Delhi` and converted city names to uppercase without spaces.\n",
    "- Converted `date_` column to a consistent `dd-mm-yyyy` format and renamed it to `DATE`.\n",
    "- Cleaned and standardized all column headers to uppercase and removed spaces.\n",
    "- Removed unnecessary columns: `UNNAMED: 0.2`, `UNNAMED: 0.1`, and `UNNAMED: 0`.\n",
    "- Checked and confirmed appropriate data types (dates as datetime, numeric columns as float/int).\n",
    "- Saved final cleaned dataset as `Sales_sample_cleaned.csv`.\n"
   ],
   "id": "dcfaeae79245f06a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
